\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}%Veröffentlichungssprache
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage[format=plain,justification=RaggedRight,singlelinecheck=false,font={small},labelsep=space]{caption}
\usepackage[dvipsnames]{xcolor}	
\usepackage[a4paper]{geometry}
	\geometry{left=3.5cm,right=2.5cm,top=2.4cm,bottom=2cm}%Seitenränder
	\usepackage[onehalfspacing]{setspace}%Zeilenabstand
	\renewcommand{\\}{\vspace*{0.5\baselineskip} \newline}
\renewcommand*\MakeUppercase[1]{#1}	
\usepackage{tablefootnote}
\usepackage{fancyhdr}
	\pagestyle{fancy}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\footrulewidth}{0pt}
	\fancyhead[R]{\footnotesize{\thepage}}
	\fancyhead[L]{\footnotesize{\leftmark}}
	\fancyfoot{}
\usepackage[colorlinks,
pdfpagelabels,
pdfstartview = FitH,
bookmarksopen = true,
bookmarksnumbered = true,
linkcolor = black,
urlcolor = black,
plainpages = false,
hypertexnames = false,
citecolor = black] {hyperref}
\usepackage[parfill]{parskip}
\usepackage{listings}
\usepackage[
	backend=biber,
	style=apa
]{biblatex}
\addbibresource{refs.bib}

\renewcommand{\familydefault}{\sfdefault}

\begin{document}

\title{Code signatures for plagiarism detection}
\author{Dennis Goßler \and Dennis Wäckerle}
\maketitle

\section*{Abstract}
\newpage
\tableofcontents
\newpage

\section{Introduction}

\section{How Does Plagiarism Detection Work}


*Fand den Abschnitt bei moss ganz nett könnte man ja mit reinpacken* Dennis G.

Moss and other plagiarism detection tools are not perfect, so a human should go other the results, and it should be checked if the clams are valid.
"In particular, it is a misuse of Moss to rely solely on the similarity scores. These scores are useful for judging the relative amount of matching between different pairs of programs and for more easily seeing which pairs of programs stick out with unusual amounts of matching. But the scores are certainly not a proof of plagiarism. Someone must still look at the code."
\autocite{SMOSS}

\section{Use Case and Software Experiment}

\subsection{Use Case}

\subsection{Software Experiment}

\section{Criteria for Evaluation}

\section{Evaluation of the different Tools}


\begin{table}[h]
	\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
	\hline
			& C   & C++ & C\# & Java & Kotlin & Python & PHP & VB.net & Javascript & Is expandable? \\ \hline
	MOSS    & Yes & Yes & Yes & Yes  & No     & Yes    & No  & Yes    & Yes        & No             \\ \hline
	JPlag   & No  & Yes & Yes & Yes  & Yes    & Yes    & No  & No     & No         & Yes            \\ \hline
	Plaggie & No  & No  & No  & Yes  & No     & No     & No  & No     & No         & Yes\tablefootnote{Plaggie is open source, therefore a custom tokenizer for different languages can be implemented.}            \\ \hline
	AC2     & Yes & Yes & No  & Yes  & No     & Yes    & Yes & No     & No         & Yes             \\ \hline
	\end{tabular}
	\caption{\label{tab:table-name}[The native supported programming languages for each plagiarism detection algorithm]}
\end{table}




\subsection{MOSS}

Moss clams to be one of the best cheating detection algorithms.
"The algorithm behind moss is a significant improvement over other cheating detection algorithms (at least, over those known to us)."
\autocite{SMOSS}

Moss supports the following programming languages C, C++, Java, C\#, Python, Visual Basic, Javascript, FORTRAN, ML, Haskell, Lisp, Scheme, Pascal, Modula2, Ada, Perl, TCL, Matlab, VHDL, Verilog, Spice, MIPS assembly, a8086 assembly, a8086 assembly and HCL2

\subsection{JPlag}

\subsubsection{JPlag's comparison algorithm}

- Functions in two phases 
1. all programs are parsed and converted into tokens
2. tokens strings are compared in pairs. Tries to cover one token stream with substrings of the other token. Percentage of covered token streams is
the similarity. \autocite[p. 10]{JPlagP}

- Tokenizing -> only language dependent process\autocite[p. 10]{JPlagP}
- Tokens represent syntactic elements e.g. statements or control structures\autocite[How are submissions represented? — Notion of Token]{JPlagW4}

- Transformation
- each file is parsed -> result set of Abstract Syntax Trees for each submission
- each AST traversed depth first, nodes are  grammatical units of language
- when entering and exiting a node a token can be created and added to the token list
- block type node e.g. classes or if expressions have corresponding begin and end tokens. 
	Token list should have balanced pairs of matching begin and end tokens \autocite[How does the transformation work?]{JPlagW4}

- Comparing token strings
- Essentially Greedy String Tiling
- When comparing two strings A and B, the same subsstrings should be found satisfying following rules
1. "Any token of A may only be matched with exactly one token from B."
2. "Substrings are to be found independent of their position in the string."
3. "Long substring matches are preferred over short ones" \autocite[p. 11]{JPlagP}
- Applying the third rule sequentially leads to a greedy algorithm consisting of two phases
1. Two strings are searched for the biggest contigous matches. 3 nested Loops. Outer loop iterates over tokens in
	string A. Second loop compares Token T to Tokens in String B. Inner loop, if identical Tokens, extends the match
	as far as possible. Collects Set of longest common substrings
2. Marks all matches of maximal length. All tokens are marked and can not be used for further matches. Satisfies rule 1
- Repeat until no further matches found \autocite[p. 11]{JPlagP}
- tile is a unique and permanent association of a substring from A with a matching substring from B\autocite[p. 3]{GST}
- MinimumMatchLength is defined must be atleast 1, should probably be higher, since it is unlikely that such a match
	will be significant \autocite[GST]{GST}

-similarity considers 100\% if the shorter string is completly covered \autocite[p. 13]{JPlagP}(See maths formula)

\subsubsection{The results}

-comparisons without base code
-successfully discovered plagiarism for m0 91.49\% and 90.51\% match
-Other comparisons around 40\%
-need base code

-m4 less clear 84.77\%-76.2\%
-need base code and another submission

\subsubsection{Integration into an automated evaluation pipeline}

-Can be executed locally
-Has a cli and an api -> allows automation
-Visualization in external web application -> might pose a problem

-known unsuccessful attacks
- Changing the comments
- Changing the indentation
- Method and variable name changes\autocite[Known successful attacks]{RMP}

-know successful attacks
- Moving inline code to separate methods and vice versa
- Inclusion of redundant program code
- Changing the order of if-else blocks and case-blocks\autocite[Known unsuccessful attacks]{RMP}

\subsection{Plaggie}

-Was the only open source tool when release\autocite{PLAGGIE}, JPlag now also open source

\subsubsection{The Algorithm}

-Uses CUP as a Parser
-Uses standard LALR(1) parser generation\autocite{CUP}
-GST with no special optimization attempts
-Algorithm was extended to support exclusion of common code\autocite[4. Algorithm used]{RMP}

\subsubsection{The results}

-TODO still have to run it.

\subsubsection{Integration into an automated evaluation pipeline}

-only Java 1.5\autocite{PLAGGIE,RMP}
-no further development since 2006
-has a cli -> can be automated
-results as html files

\subsection{AC2}

"AC was born in the Escuela Politécnica Superior of the Universidad Autónoma de Madrid to deter and detect source-code plagiarism in programming assignments." \autocite{AC2}


\subsubsection{AC2 comparison algorithm}

The output that AC2 generates is visualization base. So AC it will not provide a value like "percentage of copy" instead, "it will create graphical representations of the degree of similarity between student submissions within a group" \autocite{AC2}   


\subsubsection{The results}

\subsubsection{Integration into an automated evaluation pipeline}



\section{Conclusion}

\newpage

\printbibliography[
	heading=bibintoc,
	title={References}
]

\appendix

\end{document}
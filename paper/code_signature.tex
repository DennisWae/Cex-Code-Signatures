\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}%Veröffentlichungssprache
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage[format=plain,justification=RaggedRight,singlelinecheck=false,font={small},labelsep=space]{caption}
\usepackage[dvipsnames]{xcolor}	
\usepackage[a4paper]{geometry}
	\geometry{left=3.5cm,right=2.5cm,top=2.4cm,bottom=2cm}%Seitenränder
	\usepackage[onehalfspacing]{setspace}%Zeilenabstand
	\renewcommand{\\}{\vspace*{0.5\baselineskip} \newline}
\renewcommand*\MakeUppercase[1]{#1}	
\usepackage{tablefootnote}
\usepackage{fancyhdr}
	\pagestyle{fancy}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\footrulewidth}{0pt}
	\fancyhead[R]{\footnotesize{\thepage}}
	\fancyhead[L]{\footnotesize{\leftmark}}
	\fancyfoot{}
\usepackage[colorlinks,
pdfpagelabels,
pdfstartview = FitH,
bookmarksopen = true,
bookmarksnumbered = true,
linkcolor = black,
urlcolor = black,
plainpages = false,
hypertexnames = false,
citecolor = black] {hyperref}
\usepackage[parfill]{parskip}
\usepackage{listings}
\usepackage[
	backend=biber,
	style=apa
]{biblatex}
\addbibresource{refs.bib}

\renewcommand{\familydefault}{\sfdefault}

\begin{document}

\title{How do various plagiarism detection tools differ in their detection results when applied to java/spring coding exercises}
\author{Dennis Goßler \and Dennis Wäckerle}
\maketitle

\section*{Abstract}
\newpage
\tableofcontents
\newpage

\section{Introduction}

\section{How Does Plagiarism Detection Work}


*Fand den Abschnitt bei moss ganz nett könnte man ja mit reinpacken* Dennis G.

Moss and other plagiarism detection tools are not perfect, so a human should go other the results, and it should be checked if the clams are valid.
"In particular, it is a misuse of Moss to rely solely on the similarity scores. These scores are useful for judging the relative amount of matching between different pairs of programs and for more easily seeing which pairs of programs stick out with unusual amounts of matching. But the scores are certainly not a proof of plagiarism. Someone must still look at the code."
\autocite{SMOSS}

\section{Use Case and Software Experiment}

\subsection{Use Case}

\subsection{Software Experiment}

\section{Criteria for Evaluation}

\section{Evaluation of the different Tools}


\begin{table}[h]
	\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
	\hline
			& C   & C++ & C\# & Java & Kotlin & Python & PHP & VB.net & Javascript & Is expandable? \\ \hline
	MOSS    & Yes & Yes & Yes & Yes  & No     & Yes    & No  & Yes    & Yes        & No             \\ \hline
	JPlag   & No  & Yes & Yes & Yes  & Yes    & Yes    & No  & No     & No         & Yes            \\ \hline
	Plaggie & No  & No  & No  & Yes  & No     & No     & No  & No     & No         & Yes\tablefootnote{Plaggie is open source, therefore a custom tokenizer for different languages can be implemented.}            \\ \hline
	AC2     & Yes & Yes & No  & Yes  & No     & Yes    & Yes & No     & No         & Yes             \\ \hline
	\end{tabular}
	\caption{\label{tab:AlgorithmLanguageSupportTable}[The native supported programming languages for each plagiarism detection algorithm]}
\end{table}


\newpage

\subsection{MOSS}

Moss clams to be one of the best cheating detection algorithms.
"The algorithm behind moss is a significant improvement over other cheating detection algorithms (at least, over those known to us)."
\autocite{SMOSS}

Moss supports the following programming languages C, C++, Java, C\#, Python, Visual Basic, Javascript, FORTRAN, ML, Haskell, Lisp, Scheme, Pascal, Modula2, Ada, Perl, TCL, Matlab, VHDL, Verilog, Spice, MIPS assembly, a8086 assembly, a8086 assembly and HCL2

\subsubsection{Moss comparison algorithm}

\subsubsection{The results}

\subsubsection{Integration into an automated evaluation pipeline}

\newpage

\subsection{JPlag}

JPlag is a plagiarism detection tool, which was developed by in 2000 at the University of Karlsruhe to help with the detection of plagiarized coding
exercises. At that the tool was only available as a WWW service and could analyze C, C++. Scheme and Java programs \autocite[p. 4]{JPlagP}. Currently, the tool is available
as an open source application which can be run locally and can be used with a CLI or a Java API. JPlag also currently supports 12 programming languages
including the original four languages and more modern languages such as Kotlin \autocite[Supported Languages
]{JPlagG} while also allowing to add new languages \autocite{JPlagW4}.

\subsubsection{JPlag's comparison algorithm}

JPlag's algorithm is split into two parts the tokenizing and the comparison of the token strings. During the tokenization
process all programs are parsed and converted into token strings. In the second phase all token strings are compared
in pairs to determine their similarity. This comparison uses a specially optimized version of the Greedy String Tiling
algorithm. This algorithm tries to cover one token string with substrings of the other string. The percentage of the covered token 
strings is the similarity between the two programs \autocite[p. 10]{JPlagP}.

\paragraph{Tokenization}

The tokenization process is the only language dependent process of the JPlag algorithm \autocite[p. 10]{JPlagP}. The extracted tokens
represent syntactic elements of the language like statements or control structures \autocite[How are submissions represented? — Notion of Token]{JPlagW4}.
During the parsing process each file is parsed with the result being a set of abstract syntax trees(AST) for each submission. Each AST
is traversed depth first with nodes representing grammatical units of the language. During the traversal when entering and exiting a 
node a token can be created that match the type of the node and will then be added to the current list of tokens. There are block type
nodes which can represent classes, if expressions or other elements of the language which have a corresponding beginning and end. When
creating tokens from those nodes each have a corresponding "BEGIN" and "END" token. The token list should always have a pair of matching
"BEGIN" and "END" tokens \autocite[How does the transformation work?]{JPlagW4}.

\paragraph{Comparing token strings}

JPlag's comparison algorithm is essentially just the greedy string tiling algorithm for the comparison of two strings, however it is differently
optimized to improve its runtime\autocite[p. 5]{JPlagP}. The goal of the algorithm is to find a set of substrings
which are not only the same but also satisfy these three rules:

\begin{enumerate}
	\item "Any token of A may only be matched with exactly one token from B."
	\item "Substrings are to be found independent of their position in the string."
	\item "Long substring matches are preferred over short ones[...]"
\end{enumerate}\autocite[p. 11]{JPlagP}

These rule have some consequences. The first rule doesn't allow the matching of code that have been duplicated while
the second rule makes the reordering of the source code not viable. Furthermore, the third rule is introduced since
short matches are more likely to be spurious \autocite[p. 11]{JPlagP}.

When these rule 3 is applied sequentially then a greedy algorithm consisting of two phases will be created.

In phase 1 the longest contiguous match in the two strings is searched. This is done by 3 nested loops. The outer loop iterates over all extracted
tokens in string A, the second loop compares token T from string A with every token in string B. If two tokens are identical then the innermost loop
extends this match as far as possible \autocite[p. 11]{JPlagP}. These matches are called tiles which are permanent and unique \autocite[p. 3]{GST}.

In the second phase all matches of the maximal length are marked, when this happens all matched tokens are also marked and can't be used for any
further matches. This satisfies the first rule. These two phases will be repeated until not further matches are found \autocite[p. 11]{JPlagP}.
Also, of importance is the MinimumMatchLength, which defines the length at which maximal matches and any tile below the maximum matches are ignored.
The smallest possible value for MinimumMatchLength is 1. However, in general this value should be higher than 1 since at that length it is unlikely
that a match will be significant \autocite[p. 3]{GST}

JPlag only considers two strings to match 100\% when the shorter string is completely matched by the longer string. The logic being that the longer string
represents a program which has been entirely copied and then extended. 
The similarity between two programs can be calculated with the following formula.
\[
	sim(A,B) = \frac{2+coverage(tiles)}{|A|+|B|}
\]
\[
	coverage(tiles) = \sum_{match(a,b,length)\in tiles} length
\]
\autocite[p. 13]{JPlagP}.

{\color{red} -Detail the specific optimizations}

\subsubsection{Preliminary results}

When using JPlag to check the similarity of known plagiarized code it had similarity value for the plagiarized m0 code of 91.49\& and 90.51\% while the
none plagiarized code had similarity of around 40\%.
\begin{figure}[h]
	\includegraphics[scale=0.4]{figs/JPlag/JPlag_Overview.png}
	\caption{The JPlag overview showing the similarities of m0 code}
\end{figure}
The results for m4 were somewhat less cleat since they had a similarity value of 84.77\& and 76.2\%. Which however would still lead to an investigation
of the submitted code.

It should also be noted that no files were excluded and that there was no base code passed to JPlag, which explains the high similarity between unrelated
programs as all submissions for a milestone use the same base code.

{\color{red} 
-need base code for m0
-need base code and another submission for m4
}

\subsubsection{Integration into an automated evaluation pipeline}

JPlag probably show the most potential to be used in an automated pipeline as it not only provides a CLI but also a Java API, while also being able to be
run locally\autocite{JPlagG} which means that there are no submission limits like in MOSS and that JPlag can be easily integrated into the existing pipeline.
However, results are always saved as a Zip file and are view in a web application which can also be run locally. This poses a problem as???

-How does the pipeline work, are files permanently saved or deleted after it finishes evaluation, can they be accessed

\subsection{Plaggie}

-Was the only open source tool when release\autocite{PLAGGIE}, JPlag now also open source

\subsubsection{The Algorithm}

-Uses CUP as a Parser
-Uses standard LALR(1) parser generation\autocite{CUP}
-GST with no special optimization attempts
-Algorithm was extended to support exclusion of common code\autocite[4. Algorithm used]{RMP}

-known unsuccessful attacks
- Changing the comments
- Changing the indentation
- Method and variable name changes\autocite[Known successful attacks]{RMP}

-know successful attacks
- Moving inline code to separate methods and vice versa
- Inclusion of redundant program code
- Changing the order of if-else blocks and case-blocks\autocite[Known unsuccessful attacks]{RMP}

\subsubsection{The results}

-TODO still have to run it.

\subsubsection{Integration into an automated evaluation pipeline}

-only Java 1.5\autocite{PLAGGIE,RMP}
-no further development since 2006
-has a cli -> can be automated
-results as html files

\newpage

\subsection{AC2}

"AC is a source code plagiarism detection tool. It aids instructors and graders to detect plagiarism within a group of assignments written in languages such as C, C++, Java, PHP, XML, Python, ECMAScript, Pascal or VHDL (plaintext works too, but is less precise). AC incorporates multiple similarity detection algorithms found in the scientific literature, and allows their results to be visualized graphically." \autocite{AC2} The first version of "AC was born in the Escuela Politécnica Superior of the Universidad Autónoma de Madrid to deter and detect source-code plagiarism in programming assignments". \autocite{AC2}

2016 AC version 2 was released. In the version 2.0 the developers switched to Maven and Antr4\footnote{\label{footnoteAntlr}"In computer-based language recognition, ANTLR (pronounced antler), or ANother Tool for Language Recognition, is a parser generator that uses LL(*) for parsing. ANTLR is the successor to the Purdue Compiler Construction Tool Set (PCCTS), first developed in 1989, and is under active development."\autocite{enwiki:1115716413}}. The developers of AC also moved their git repository to GitHub an internet hosting service for software development and version control.

AC2 lists some advantages over the other plagiarism detection tools on their GitHub page. In this regard, it is

\begin{itemize}
	\item "local, and does not require sending data to remote servers (not the case of Moss)local, and does not require sending data to remote servers (not the case of Moss)"
	\item "robust, using Normalized Compression Distance as its main measure-of-similarity, but with the possibility of integrating other, additional measures to gain better pictures of what is going on (JPlag, Moss and Plaggie have hard-coded analyses, mostly based on sub-string matching after tokenization)."
	\item "heavy on information visualization. AC will not provide "percentage of copy"; instead, it will create graphical representations of the degree of similarity between student submissions within a group, so instructors can build their own explanations regarding what really happened (see here and here for papers on AC's visualizations)."
\end{itemize}\autocite{AC2}

\subsubsection{AC2 comparison algorithm}
\label{sec:AC2ComparisonAlgorithm}

In order for the AC algorithm to check files for equality, it allows adding folders or zip-files into it. Two filters can be applied to these files. The first filter selects the submissions that will be examined, and the second filter is determining which files are compared to each other. 

For example student s1 has in his subfolder 'a.c' and student s2 has in his folder 'aa.c'. So the first filter should determine that each submission starts with the letter 's'. The second filter would look for files that ended with the string '.c'.

When all submission and files are correctly grouped together the analysis part can start. "The default analysis (Zip NCD), uses \hyperref[sec:NCD]{normalized compression distance (NCD)} to detect redundancy (= similarity) between submissions. If AC2 knows the programming language of its source files, they will be tokenized prior to comparison. This will allow AC2 to ignore comments, whitespace and exact identifiers, so that simply renaming variables or altering formatting will not affect similarity calculations." \autocite{AC2wiki} 

\paragraph{NCD (normalized compression distance)}
\label{sec:NCD}

\textcolor{red}{Todo: NCD beschreiben und sagen wie es in AC2 verwendet wird.}

\paragraph{Tokenization}

If AC2 can recognize the programming language it will use a lexer and a parser to tokenize the text file. "AC2 uses Antlr4\footref{footnoteAntlr} grammars to generate lexers (= tokenizers) and parsers for languages." \autocite{AC2wiki} Therefore it is relatively easy to add support for a new programming language.

\subsubsection{Preliminary results}

In order to present the advantages of AC2, several projects were compared using the AC2 algorithm. As explained in section~\ref{sec:AC2ComparisonAlgorithm} \nameref{sec:AC2ComparisonAlgorithm}, the filters were used to select and group the submissions. After passing the project files, the algorithm analyzes them and creates an output window. The output that AC2 generates is mainly visual based. So Ac "will create graphical representations of the degree of similarity between student submissions within a group". \autocite{AC2}. See figure~\ref{fig:AC2R1} and figure~\ref{fig:AC2R2}.

\begin{figure}[h]
	\centering
	\includegraphics[height=0.2\textheight]{figs/AC2/M0_1.png}
	\caption{The graphical output of AC2 from milestone [0]}
	\label{fig:AC2R1}
\end{figure}
Figure~\ref{fig:AC2R1} shows milestone 0 and how the different projects have copied from each other. This suggests that project N1 has maybe copied its code form G2 and N2 from G2.

\begin{figure}[h]
	\centering
		\includegraphics[height=0.2\textheight]{figs/AC2/M4_1.png}
		\caption{The graphical output of AC2 from milestone [4]}
		\label{fig:AC2R2}
\end{figure}

Figure~\ref{fig:AC2R2} illustrates that G1, N1 and N2 have probably copied from each other, since a correlation exists between the 3 projects. This again illustrates the advantage of AC2 over the other plagiarism detection programs, since it is easier to detect multiple correlations between projects.

\begin{table}[h]
	\begin{minipage}{.5\textwidth}
		\centering
		\scalebox{0.9}{
			\begin{tabular}{{|l|l|l|}}
				\hline
				One           & The other     & Distance \\ \hline
				ST2M0-Geber1  & ST2M0-Nehmer1 & 0.2926   \\ \hline
				ST2M0-Geber2  & ST2M0-Nehmer2 & 0.4609   \\ \hline
				ST2M0-Geber1  & ST2M0-Nehmer2 & 0.6302   \\ \hline
				ST2M0-Nehmer2 & ST2M0-Nehmer1 & 0.6705   \\ \hline
				ST2M0-Geber2  & ST2M0-Geber1  & 0.6718   \\ \hline
				ST2M0-Geber2  & ST2M0-Nehmer1 & 0.6811   \\ \hline
			\end{tabular}
		}
		\caption{\label{tab:AC2DistanceTableM0}[Table of Milestone [0] distance between projects]}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\scalebox{0.9}{
			\begin{tabular}{|l|l|l|}
				\hline
				One           & The other     & Distance \\ \hline
				ST2M4-Geber1  & ST2M4-Nehmer2 & 0.6538   \\ \hline
				ST2M4-Geber1  & ST2M4-Nehmer1 & 0.6603   \\ \hline
				ST2M4-Nehmer2 & ST2M4-Nehmer1 & 0.6957   \\ \hline
				ST2M4-Geber2  & ST2M4-Nehmer1 & 0.6809   \\ \hline
				ST2M4-Geber1  & ST2M4-Geber3  & 0.8647   \\ \hline
				ST2M4-Geber3  & ST2M4-Nehmer1 & 0.8657   \\ \hline
				ST2M4-Geber2  & ST2M4-Geber3  & 0.8772   \\ \hline
				ST2M4-Geber3  & ST2M4-Nehmer2 & 0.8792   \\ \hline
				ST2M4-Geber2  & ST2M4-Nehmer2 & 0.8809   \\ \hline
				ST2M4-Geber2  & ST2M4-Geber1  & 0.8823   \\ \hline
			
			\end{tabular}
		}
		\caption{\label{tab:AC2DistanceTableM4}[Table of Milestone [4] distance between projects]}
	\end{minipage}
\end{table}

Table~\ref{tab:AC2DistanceTableM0} and table~\ref{tab:AC2DistanceTableM4} show how far apart the individual projects are. This distinguishes AC2 from other plagiarism detection programs, as they often reflect the similarity in percent to the user.

\newpage


\section{Conclusion}

\newpage

\printbibliography[
	heading=bibintoc,
	title={References}
]

\appendix

\end{document}